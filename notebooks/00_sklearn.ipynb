{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac9b286",
   "metadata": {},
   "source": [
    "\n",
    "# What scikit-learn allows you to do?\n",
    "\n",
    "No better way than to show an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5864c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdadb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe473f",
   "metadata": {},
   "source": [
    "\n",
    "The idea is to predict the median house value from the other features.\n",
    "\n",
    "Scikit-learn allows you to design, evaluate, and tune predictive models on tabular\n",
    "data. So we could quickly try a linear model and split the dataset into a training\n",
    "and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(data_train, target_train)\n",
    "score = ridge.score(data_test, target_test)\n",
    "print(f\"R2 score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593f333",
   "metadata": {},
   "source": [
    "\n",
    "But we know that we should not do only this. We have no clue regarding the\n",
    "variance of the model. So we have tools for cross-validation to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc95a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "cv_results = cross_validate(\n",
    "    ridge, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[[\"train_score\", \"test_score\"]].aggregate([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025d44e",
   "metadata": {},
   "source": [
    "\n",
    "Most probably, we should have started by some exploratory data analysis. We would have\n",
    "noticed that there is not a linear relationship between the features and the target.\n",
    "\n",
    "So the baseline model is not good enough. We would need to make some \"feature\n",
    "engineering\" to improve the model.\n",
    "\n",
    "We have some nice tools to make subsequent processing easier: (i) it applies the\n",
    "expected transformations to the data and (ii) it stores **states** of transformers\n",
    "at fit time such that it can be used later to transform new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fb21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SplineTransformer(),\n",
    "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=True),\n",
    "    Ridge(),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    model, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]].aggregate([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3e3e3",
   "metadata": {},
   "source": [
    "\n",
    "But if we look closer at the data, it seems that we should apply different\n",
    "transformations to different features. Indeed, the latitude and longitude are\n",
    "geographical coordinates and we could create some clusters representing biggest\n",
    "cities.\n",
    "\n",
    "On the other hand, the other features are numerical and could be better\n",
    "understood by the model if we transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "geo_columns = [\"Latitude\", \"Longitude\"]\n",
    "spline_columns = [\"MedInc\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\"]\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (KMeans(n_clusters=10), geo_columns),\n",
    "    (make_pipeline(StandardScaler(), SplineTransformer()), spline_columns),\n",
    ")\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=True),\n",
    "    Ridge(),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    model, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]].aggregate([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ec67d",
   "metadata": {},
   "source": [
    "\n",
    "And finally, you have tools to help at tuning the hyperparameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72265012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (KMeans(n_clusters=10), geo_columns),\n",
    "    (make_pipeline(StandardScaler(), SplineTransformer()), spline_columns),\n",
    ")\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    PolynomialFeatures(degree=1, include_bias=False, interaction_only=True),\n",
    "    SelectKBest(k=30),\n",
    "    RidgeCV(alphas=np.logspace(-5, 5, num=50)),\n",
    ")\n",
    "param_distributions = {\n",
    "    \"columntransformer__kmeans__n_clusters\": randint(2, 30),\n",
    "    \"columntransformer__pipeline__splinetransformer__n_knots\": randint(2, 10),\n",
    "    \"polynomialfeatures__degree\": [1, 2],\n",
    "    \"selectkbest__k\": randint(50, 1000),\n",
    "}\n",
    "search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, cv=5, n_iter=10, verbose=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "cv_results_path = Path(\"../data/00_search_cv.joblib\")\n",
    "\n",
    "# It is costly, let's reload from the disk if it exists\n",
    "if cv_results_path.exists():\n",
    "    cv_results = joblib.load(cv_results_path)\n",
    "else:\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        cv_results = cross_validate(\n",
    "            search, data, target, cv=cv, return_estimator=True, return_train_score=True\n",
    "        )\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    joblib.dump(cv_results, cv_results_path)\n",
    "cv_results[[\"train_score\", \"test_score\"]].aggregate([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in cv_results[\"estimator\"]:\n",
    "    print(est.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f496b38",
   "metadata": {},
   "source": [
    "\n",
    "And finally, we have tools to help us understand the model via displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78059a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "for est, ax in zip_longest(cv_results[\"estimator\"], axs.ravel()):\n",
    "    if est is None:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    PredictionErrorDisplay.from_estimator(\n",
    "        est, data_test, target_test, kind=\"actual_vs_predicted\", ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"R2 score: {est.score(data_test, target_test):.2f}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a533f",
   "metadata": {},
   "source": [
    "\n",
    "Bonus point: you can dump the model and use it in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95290aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search.fit(data, target)\n",
    "# joblib.dump(search.best_estimator_, \"../models/00_my_production_model.joblib\")\n",
    "# prod_model = joblib.load(\"../models/00_my_production_model.joblib\")\n",
    "# prod_model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef77d50",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusions\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- Simple consistent API\n",
    "- A lot of building block to build and tune your predictive model\n",
    "- A lot of tools to evaluate your predictive model\n",
    "- A lot of tools to inspect your predictive model\n",
    "- Robust and fast implementation\n",
    "- Good documentation\n",
    "\n",
    "### Pitfalls\n",
    "\n",
    "**From the demo**\n",
    "- By nature, scikit-learn offers generic components\n",
    "- Know-how is extremely important\n",
    "  - No available baseline to start with\n",
    "  - Some syntax are convoluted\n",
    "  - Some choices to be made require expertise\n",
    "  - One can make methodological errors\n",
    "\n",
    "**What we did not show**\n",
    "- Data preprocessing is actually hard\n",
    "  - Data can come from different sources\n",
    "  - Transformations are not necessarily standardized\n",
    "- What happens once predictive models are in production\n",
    "  - Pickling and security\n",
    "  - Documentation\n",
    "  - Registry"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
